{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "We will be creating an argumentative chatbot based on this dataset: https://nlds.soe.ucsc.edu/iac "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create the raw input-output pairs from the dataset \n",
    "\n",
    "We begin by converting the arguments dataset into a format useful to us. The dataset contains various features, but we're primarily concerned with creating parent-child (statement-response or input-output) post pairs. Because the posts are not necessarily replying to their direct predecessor in the discussion (and thus in the JSON), we will have to keep track of the post id and parent id for each post to create pairings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_text = {}\n",
    "parent_to_child_ids = []\n",
    "my_data_dir = \"./data/\"\n",
    "my_data_dir_full = os.path.join(os.getcwd(), my_data_dir)\n",
    "if not os.path.isdir(my_data_dir_full):\n",
    "    os.mkdir(my_data_dir_full)\n",
    "build_pairs = True\n",
    "\n",
    "if build_pairs:\n",
    "    data_dir = \"./iac_v1.1/data/fourforums/discussions/\"\n",
    "    \n",
    "    discussion_filepaths = [os.path.join(data_dir, file) for file in os.listdir(data_dir)]\n",
    "\n",
    "    print(\"Building metadata (Part 1/2)...\")\n",
    "    curr_file_num = 0\n",
    "    start_time = time.time()\n",
    "    for discussion_filepath in discussion_filepaths:\n",
    "        if curr_file_num % (len(discussion_filepaths)//10) == 0:\n",
    "            print(\"Processing file {} of {}...\".format(curr_file_num, len(discussion_filepaths)))\n",
    "        curr_file_num += 1\n",
    "        with open(discussion_filepath, 'r') as discussion_file:\n",
    "            discussion = json.load(discussion_file)[0]\n",
    "            for post in discussion:\n",
    "                post_id = post[0]\n",
    "                post_parent_id = post[5]\n",
    "                post_text = post[3]\n",
    "                id_to_text[post_id] = post_text\n",
    "                if post_parent_id:\n",
    "                    parent_to_child_ids.append((post_parent_id, post_id))\n",
    "\n",
    "    print(\"Saving metadata (Part 2/2)...\")\n",
    "    with open(os.path.join(my_data_dir, 'id_to_text.pkl'), 'wb') as id_to_text_file:\n",
    "        pickle.dump(id_to_text, id_to_text_file, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(os.path.join(my_data_dir, 'parent_to_child_ids.pkl'), 'wb') as parent_to_child_ids_file:\n",
    "        pickle.dump(parent_to_child_ids, parent_to_child_ids_file, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(\"Finished processing files!\\nTotal elapsed time: {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convert raw input-output pairs into feedable format for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(my_data_dir, 'id_to_text.pkl'), 'rb') as id_to_text_file:\n",
    "    id_to_text = pickle.load(id_to_text_file)\n",
    "\n",
    "with open(os.path.join(my_data_dir, 'parent_to_child_ids.pkl'), 'rb') as parent_to_child_ids_file:\n",
    "    parent_to_child_ids = pickle.load(parent_to_child_ids_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_lengths = {post_id: len(post.split(' ')) for post_id, post  in id_to_text.items()}\n",
    "\n",
    "print(\"Mean post length: {}\".format(np.mean(list(post_lengths.values()))))\n",
    "print(\"Min post length: {}\".format(np.min(list(post_lengths.values()))))\n",
    "print(\"Max post length: {}\".format(np.max(list(post_lengths.values()))))\n",
    "print(\"Post length standard deviation: {}\".format(np.std(list(post_lengths.values()))))\n",
    "\n",
    "print(\"Percentage of posts shorter than 450 words: {}\".format(np.mean([post_length < 450 for post_length in post_lengths.values()])))\n",
    "plt.hist(list(post_lengths.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_length_ratios = [post_lengths[parent_id]/post_lengths[child_id] for parent_id, child_id in parent_to_child_ids]\n",
    "\n",
    "print(\"Mean ratio: {}\".format(np.mean(post_length_ratios)))\n",
    "print(\"Min ratio: {}\".format(np.min(post_length_ratios)))\n",
    "print(\"Max ratio: {}\".format(np.max(post_length_ratios)))\n",
    "print(\"Ratio standard deviation: {}\".format(np.std(post_length_ratios)))\n",
    "\n",
    "print(\"Percentage of ratios greater than 4: {}\".format(np.mean([post_length_ratio > 4 for post_length_ratio in post_length_ratios])))\n",
    "print(\"Percentage of ratios less than 0.25: {}\".format(np.mean([post_length_ratio < 0.25 for post_length_ratio in post_length_ratios])))\n",
    "plt.hist(post_length_ratios)\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def light_tokenizer(post):\n",
    "    return [token.text for token in nlp(post, disable=['parser', 'tagger', 'ner'])]\n",
    "\n",
    "post_field = torchtext.data.Field(tokenize=light_tokenizer, init_token='<sos>', eos_token='<eos>', lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_csv = True\n",
    "max_post_length = 450\n",
    "max_post_length_ratio = 4\n",
    "min_post_length_ratio = 0.25\n",
    "\n",
    "if build_csv:\n",
    "    with open(os.path.join(my_data_dir, 'id_to_text.pkl'), 'rb') as id_to_text_file:\n",
    "        id_to_text = pickle.load(id_to_text_file)\n",
    "\n",
    "    with open(os.path.join(my_data_dir, 'parent_to_child_ids.pkl'), 'rb') as parent_to_child_ids_file:\n",
    "        parent_to_child_ids = pickle.load(parent_to_child_ids_file)\n",
    "\n",
    "    pairs = {'parent_post': [], 'child_post': []}\n",
    "    for parent_post_id, child_post_id in parent_to_child_ids:\n",
    "        parent_post_length = len(id_to_text[parent_post_id].split(' '))\n",
    "        child_post_length = len(id_to_text[child_post_id].split(' '))\n",
    "        \n",
    "        if parent_post_length < max_post_length and child_post_length < max_post_length:\n",
    "            # TODO: check for large difference in post length\n",
    "            pairs['parent_post'].append(id_to_text[parent_post_id])\n",
    "            pairs['child_post'].append(id_to_text[child_post_id])\n",
    "\n",
    "    print(\"Percentage of data retained: {}\".format(len(pairs['parent_post']) *100 / len(parent_to_child_ids)))\n",
    "    pairs_df = pd.DataFrame(pairs, columns=['parent_post', 'child_post'])\n",
    "    train, test = sklearn.model_selection.train_test_split(pairs_df, test_size=0.2)\n",
    "\n",
    "    train.to_csv(os.path.join(my_data_dir, 'train.csv'), index=False)\n",
    "    test.to_csv(os.path.join(my_data_dir, 'test.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building datasets...\")\n",
    "dataset_start_time = time.time()\n",
    "pair_fields = [('parent_post', post_field), ('child_post', post_field)]\n",
    "train_set, test_set = torchtext.data.TabularDataset.splits(path=my_data_dir, train='train.csv', test='test.csv', format='csv', fields=pair_fields)\n",
    "print(\"Datasets built! Time elapsed: {}\".format(time.time()-dataset_start_time))\n",
    "\n",
    "print(\"Building vocabulary...\")\n",
    "vocab_start_time = time.time()\n",
    "post_field.build_vocab(train_set, min_freq=2)\n",
    "print(\"Vocabulary built! Time elapsed: {}\".format(time.time()-vocab_start_time))\n",
    "print(\"Vocabulary size: {}\".format(len(post_field.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cpu_device = torch.device('cpu')\n",
    "train_set_iterator, test_set_iterator = torchtext.data.BucketIterator.splits((train_set, test_set), batch_size=64, device=device, shuffle=True, sort_key=lambda pair: len(pair.parent_post))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create the seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_layer_size, num_hidden_layers, dropout_rate):\n",
    "        super(Seq2SeqEncoder, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = torch.nn.LSTM(embedding_size, hidden_layer_size, num_hidden_layers, dropout=dropout_rate)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, encoder_input):\n",
    "        output, (hidden_state, cell) = self.rnn(self.dropout(self.embedding(encoder_input)))\n",
    "        return hidden_state, cell\n",
    "        \n",
    "class Seq2SeqDecoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_layer_size, num_hidden_layers, output_size, dropout_rate):\n",
    "        super(Seq2SeqDecoder, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = torch.nn.LSTM(embedding_size, hidden_layer_size, num_hidden_layers, dropout=dropout_rate)\n",
    "        self.output_size = output_size\n",
    "        self.out = torch.nn.Linear(hidden_layer_size, output_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, decoder_input, hidden_state, cell):\n",
    "        output, (hidden_state, cell) = self.rnn(self.dropout(self.embedding(decoder_input.unsqueeze(0))), (hidden_state, cell))\n",
    "        return self.out(output.squeeze(0)), hidden_state, cell \n",
    "    \n",
    "class Seq2Seq(torch.nn.Module):\n",
    "    def __init__(self, encoder_net, decoder_net):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder_net = encoder_net\n",
    "        self.decoder_net = decoder_net\n",
    "        \n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        batch_size = target.shape[1]\n",
    "        max_len = target.shape[0]\n",
    "        vocab_size = self.decoder_net.output_size\n",
    "        s2s_outputs = torch.zeros(max_len, batch_size, vocab_size).to(device)\n",
    "        \n",
    "        hidden_state, cell = self.encoder_net(source)\n",
    "        decoder_input = target[0,:]\n",
    "        for token_index in range(1, max_len):\n",
    "            output, hidden_state, cell = self.decoder_net(decoder_input, hidden_state, cell)\n",
    "            s2s_outputs[token_index] = output\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                decoder_input = target[token_index]\n",
    "            else:\n",
    "                decoder_input = output.argmax(1)\n",
    "        \n",
    "        return s2s_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = output_size = len(post_field.vocab)\n",
    "embedding_size = 256\n",
    "hidden_layer_size = 512\n",
    "num_hidden_layers = 2\n",
    "dropout_rate = 0.5\n",
    "\n",
    "encoder_net = Seq2SeqEncoder(input_size, embedding_size, hidden_layer_size, num_hidden_layers, dropout_rate)\n",
    "decoder_net = Seq2SeqDecoder(input_size, embedding_size, hidden_layer_size, num_hidden_layers, output_size, dropout_rate)\n",
    "seq2seq_net = Seq2Seq(encoder_net, decoder_net).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train the seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_net = seq2seq_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(seq2seq_net.parameters())\n",
    "padding_index = post_field.vocab.stoi['<pad>']\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=padding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_clip = 1\n",
    "def train(seq2seq_net, train_set_iterator, optimizer, loss_function, max_clip):\n",
    "    seq2seq_net.train()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_set_iterator):\n",
    "        parent_post = batch.parent_post\n",
    "        child_post = batch.child_post\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        response = seq2seq_net(parent_post, child_post, 0.5)\n",
    "        response = response[1:].view(-1, response.shape[-1])\n",
    "        child_post = child_post[1:].view(-1)\n",
    "        \n",
    "        loss = loss_function(response, child_post)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(seq2seq_net.parameters(), max_clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss/len(train_set_iterator)\n",
    "\n",
    "def test(seq2seq_net, test_set_iterator, loss_function):\n",
    "    seq2seq_net.eval()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(test_set_iterator):\n",
    "        parent_post = batch.parent_post\n",
    "        child_post = batch.child_post\n",
    "        \n",
    "        response = seq2seq_net(parent_post, child_post, 0)\n",
    "        response = response[1:].view(-1, response.shape[-1])\n",
    "        child_post = child_post[1:].view(-1)\n",
    "        \n",
    "        loss = loss_function(response, child_post)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss/len(test_set_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"v0-testing\"\n",
    "run_models_directory = os.path.join(os.getcwd(), my_data_dir, 'models/{}'.format(run_name))\n",
    "if not os.path.isdir(run_models_directory):\n",
    "    os.mkdir(run_models_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = torch.utils.tensorboard.SummaryWriter(log_dir=os.path.join(my_data_dir, \"runs/{}\".format(run_name)))\n",
    "num_epochs = 10\n",
    "\n",
    "lowest_loss_train = float('inf')\n",
    "lowest_loss_test = float('inf')\n",
    "\n",
    "print(\"Beginning training...\")\n",
    "training_start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Starting epoch {}...\".format(epoch))\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    loss_train = train(seq2seq_net, train_set_iterator, optimizer, loss_function, max_clip)\n",
    "    loss_test = test(seq2seq_net, test_set_iterator, loss_function)\n",
    "    loss_train_scalar = loss_train.detach().to(cpu_device).numpy().squeeze()\n",
    "    loss_test_scalar = loss_test.detach().to(cpu_device).numpy().squeeze()\n",
    "    writer.add_scalar('Loss/train', loss_train_scalar, epoch)\n",
    "    writer.add_scalar('Loss/test', loss_test_scalar, epoch)\n",
    "    \n",
    "    print(\"Epoch training duration: {}\".format(time.time()-epoch_start_time))\n",
    "    print(\"Updating models...\")\n",
    "    \n",
    "    if loss_train_scalar <= lowest_loss_train:\n",
    "        torch.save(seq2seq_net.state_dict(), os.path.join(my_data_dir, '/models/{}/best-model-train.pkl'.format(run_name)))\n",
    "        lowest_loss_train = loss_train_scalar\n",
    "    if loss_test_scalar <= lowest_loss_test:\n",
    "        torch.save(seq2seq_net.state_dict(), os.path.join(my_data_dir, 'models/{}/best-model-test.pkl'.format(run_name)))\n",
    "        lowest_loss_test = loss_test_scalar\n",
    "    \n",
    "    print(\"Total elapsed time: {}\".format(time.time()-training_start_time))\n",
    "    \n",
    "\n",
    "print(\"Finished training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
